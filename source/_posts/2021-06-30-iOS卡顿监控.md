---
title: iOS卡顿监控
date: 2021-06-30 19:28:45
urlname: Lag-Monitor.html
tags:
  - LagMonitor
categories:
  - 性能优化
---

## 一、卡顿的难点

时不时会收到这样的卡顿反馈：“用户A 刚才碰到从后台切换前台卡了一下，最近偶尔会遇到几次”、“用户B 反馈点对话框卡了五六秒”、“现网有用户反馈切换 tab 很卡”。

这些反馈有几个特点，导致跟进困难：

1. 不易重现。可能是特定用户的手机上才有问题，由于种种原因这个手机不能拿来调试；也有可能是特定的时机才会出问题，过后就不能重现了（例如线程抢锁）。
2. 操作路径长，日志无法准确打点

对于这些界面卡顿反馈，通常我们拿用户日志作用不大，增加日志点也用处不大。只能不断重试希望能够重现出来，或者埋头代码逻辑中试图能找的蛛丝马迹。

## 二、原理

在开始之前，我们先思考一下，界面卡顿是由哪些原因导致的？

- 死锁：主线程拿到锁 A，需要获得锁 B，而同时某个子线程拿了锁 B，需要锁 A，这样相互等待就死锁了。
- 抢锁：主线程需要访问 DB，而此时某个子线程往 DB 插入大量数据。通常抢锁的体验是偶尔卡一阵子，过会就恢复了。
- 主线程大量 IO：主线程为了方便直接写入大量数据，会导致界面卡顿。
- 主线程大量计算：算法不合理，导致主线程某个函数占用大量 CPU。
- 大量的 UI 绘制：复杂的 UI、图文混排等，带来大量的 UI 绘制。

针对这些原因，我们可以怎么定位问题呢？

- 死锁一般会伴随 crash，可以通过 crash report 来分析。
- 抢锁不好办，将锁等待时间打出来用处不大，我们还需要知道是**谁占了锁**。
- 大量 IO 可以在函数开始结束打点，将占用时间打到日志中。
- 大量计算同理可以将耗时打到日志中。
- 大量 UI 绘制一般是必现，还好办；如果是偶现的话，想加日志点都没地方，因为是**慢在系统函数里面**。

如果可以将当时的线程堆栈捕捉下来，那么上述难题都迎刃而解。主线程在什么函数哪一行卡住、在等什么锁而这个锁又是被哪个子线程的哪个函数占用、是在进行I/O操作、或者是进行复杂计算，有了堆栈，我们都可以知道。自然也能知道是慢在UI绘制，还是慢在我们的代码。

所以，思路就是**起一个子线程，监控主线程的活动情况，如果发现有卡顿，就将堆栈 dump 下来**。

流程图描述如下：

<img src="/images/PerfOpt/lag-process.png" style="zoom:85%">



## 三、细节

原理一旦讲出来，好像也不复杂。魔鬼都是隐藏在细节中，效果好不好，完全由实现细节决定。具体到卡顿检测，有几个问题需要仔细处理：

- 怎么知道主线程发生了卡顿？
- 子线程以什么样的策略和频率来检测主线程？这个是要发布到现网的，如果处理不好，带来明显的性能损耗（尤其是电量），就不能接受了。
- 堆栈上报了上来怎么分类？直接用 crash report 的分类不适合。
- 卡顿 dump 下来的堆栈会有多频繁？数据量会有多大？
- 全量上报还是抽样上报？怎么在问题跟进与节省流量直接平衡？

### 3.1 卡顿判断标准

怎么判断主线程是不是发生了卡顿？一般来说，用户感受得到的卡顿大概有三个特征：

- FPS 降低
- CPU 占用率很高
- 主线程 Runloop 执行了很久

看起来 FPS 能够兼容后面两个特征，但是在实际操作过程中发现 FPS 并不适用，不好衡量：

- 人眼结构上看，当一组动作在 1 秒内有 12 次变化（即 12FPS），我们会认为这组动作是连贯的；
- 平时看到的大部分电影或视频 FPS 其实不高，一般只有 25FPS ~ 30FPS，而实际上我们也没有觉得卡顿；
- 游戏玩家通常追求更流畅的游戏画面体验一般要达到 60FPS 以上
- **FPS 低并不意味着卡顿发生，而卡顿发生 FPS 一定不高**。FPS 可以衡量一个界面的流程性，但往往不能很直观的衡量卡顿的发生。

而对于抢锁或大量 IO 的情况，光有 CPU 是不行的。所以我们实际上用到的是下面两个准则：

- 单核 CPU 的占用超过了 80%
- 主线程 Runloop 执行了超过2秒

### 3.2 卡顿检测实现

在 iOS/macOS 平台应用中，主线程有一个 Runloop。Runloop 是一个 Event Loop 模型，让线程可以处于接收消息、处理事件、进入等待而不马上退出。在进入事件的前后，Runloop 会向注册的 Observer 通知相应的事件。

Runloop 的详细介绍可以参考：[深入理解RunLoop](https://blog.ibireme.com/2015/05/18/runloop/)，一个简易的 Runloop 流程如下所示：

<img src="/images/PerfOpt/simple-runloop-model.png" style="zoom:80%">

Matrix 卡顿监控在 Runloop 的起始最开始和结束最末尾位置添加 Observer，从而获得主线程的开始和结束状态。卡顿监控起一个子线程定时检查主线程的状态，当主线程的状态运行超过一定阈值则认为主线程卡顿，从而标记为一个卡顿。

<img src="/images/PerfOpt/main-thread-lag-check.png" style="zoom:80%">

目前微信使用的卡顿监控，主程序 Runloop 超时的阈值是 2 秒，子线程的检查周期是 1 秒。每隔 1 秒，子线程检查主线程的运行状态；如果检查到主线程 Runloop 运行超过 2 秒则认为是卡顿，并获得当前的线程快照。

同时，我们也认为 CPU 过高也可能导致应用出现卡顿，所以在子线程检查主线程状态的同时，如果检测到 CPU 占用过高，会捕获当前的线程快照保存到文件中。目前微信应用中认为，单核 CPU 的占用超过了 80%，此时的 CPU 占用就过高了。

代码示例：[SMLagMonitor.m](https://github.com/ming1016/DecoupleDemo/blob/master/DecoupleDemo/SMLagMonitor.m)

```objc
//创建子线程监控
dispatch_async(dispatch_get_global_queue(0, 0), ^{
    //子线程开启一个持续的 loop 用来进行监控
    while (YES) {
        long semaphoreWait = dispatch_semaphore_wait(dispatchSemaphore, dispatch_time(DISPATCH_TIME_NOW, 3 * NSEC_PER_SEC));
        if (semaphoreWait != 0) {
            if (!runLoopObserver) {
                timeoutCount = 0;
                dispatchSemaphore = 0;
                runLoopActivity = 0;
                return;
            }
            //BeforeSources 和 AfterWaiting 这两个状态能够检测到是否卡顿
            if (runLoopActivity == kCFRunLoopBeforeSources || runLoopActivity == kCFRunLoopAfterWaiting) {
                //将堆栈信息上报服务器的代码放到这里
            } //end activity
        }// end semaphore wait
        timeoutCount = 0;
    }// end while
});
```

### 3.3 检测策略—退火算法

为了降低检测带来的性能损耗，我们仔细设计了检测线程的策略：

- 内存 dump：每次子线程检查到主线程卡顿，会先获得主线程的堆栈并保存到内存中（不会直接去获得线程快照保存到文件中）；
- 文件 dump：将获得的主线程堆栈与上次卡顿获得的主线程堆栈进行比对：
  - 如果堆栈不同，则获得当前的线程快照并写入文件中；
  - 如果相同则会跳过，并按照斐波那契数列将**检查时间递增**（1，1，2，3，5，8…）直到没有遇到卡顿或者主线程卡顿堆栈不一样。

这样，可以避免同一个卡顿写入多个文件的情况；避免检测线程遇到主线程卡死的情况下，不断写线程快照文件。

### 3.4 卡顿时堆栈获取

#### 3.4.1 直接调用系统函数

获取堆栈信息的一种方法是直接调用系统函数。这种方法的优点在于，性能消耗小。但是，它只能够获取简单的信息，也没有办法配合 dSYM 来获取具体是哪行代码出了问题，而且能够获取的信息类型也有限。这种方法，因为性能比较好，所以适用于观察大盘统计卡顿情况，而不是想要找到卡顿原因的场景。

直接调用系统函数方法的主要思路是：用 signal 进行错误信息的获取。具体代码如下

```objc
static int s_fatal_signals[] = {
    SIGABRT,
    SIGBUS,
    SIGFPE,
    SIGILL,
    SIGSEGV,
    SIGTRAP,
    SIGTERM,
    SIGKILL,
};

static int s_fatal_signal_num = sizeof(s_fatal_signals) / sizeof(s_fatal_signals[0]);

void UncaughtExceptionHandler(NSException *exception) {
    NSArray *exceptionArray = [exception callStackSymbols]; //得到当前调用栈信息
    NSString *exceptionReason = [exception reason];       //非常重要，就是崩溃的原因
    NSString *exceptionName = [exception name];           //异常类型
}

void SignalHandler(int code){
    NSLog(@"signal handler = %d",code);
}

void InitCrashReport(){
    //系统错误信号捕获
    for (int i = 0; i < s_fatal_signal_num; ++i) {
        signal(s_fatal_signals[i], SignalHandler);
    }
    //oc未捕获异常的捕获
    NSSetUncaughtExceptionHandler(&UncaughtExceptionHandler);
}

int main(int argc, char * argv[]) {
    @autoreleasepool {
        InitCrashReport();
        return UIApplicationMain(argc, argv, nil, NSStringFromClass([AppDelegate class]));
```

#### 3.4.2 PLCrashReporter三方库

PLCrashReporter是一个开源的第三方框架，用来做 crash 收集，可以直接用 PLCrashReporter 来获取堆栈信息。这种方法的特点是，能够定位到问题代码的具体位置，而且性能消耗也不大。所以，也是我推荐的获取堆栈信息的方法。

具体如何使用 PLCrashReporter 来获取堆栈信息，代码如下所示：

```objc
// 获取数据
NSData *lagData = [
  								 [[PLCrashReporter alloc] initWithConfiguration:
                     [[PLCrashReporterConfig alloc]
                      initWithSignalHandlerType:PLCrashReporterSignalHandlerTypeBSD symbolicationStrategy:PLCrashReporterSymbolicationStrategyAll]]
                   generateLiveReport];
// 转换成 PLCrashReport 对象
PLCrashReport *lagReport = [[PLCrashReport alloc] initWithData:lagData error:NULL];
// 进行字符串格式化处理
NSString *lagReportString = [PLCrashReportTextFormatter stringValueForCrashReport:lagReport withTextFormat:PLCrashReportTextFormatiOS];
//将字符串上传服务器
NSLog(@"lag happen, detail below: \n %@",lagReportString);
```

#### 3.4.3 KSCrash

KSCrash 是 iOS 上一个知名的 crash 收集框架。包括腾讯开源的 APM 框架 Matrix，其中 crash 收集部分也是直接使用的 KSCrash。

KSCrash 可以处理以下类型的崩溃：

- Mach kernel exceptions Mac内核异常
- Fatal signals
- C++ exceptions
- Objective-C exceptions
- Main thread deadlock (experimental)  主线程死锁
- Custom crashes (e.g. from scripting languages) 自定义崩溃

#### 3.4.4 WCCrashBlockMonitorPlugin

Matrix for iOS/macOS 是微信开源的一个工具，可以使用在 iOS、macOS 平台上。在日常开发中，微信iOS团队通过卡顿监控上报的堆栈，找到微信的代码不合理之处或者是一些性能瓶颈；通过卡顿监控的辅助，尽可能地提升微信的流畅性，给用户带来更加极致美好的体验。

工具监控范围包括：崩溃、卡顿和爆内存，包含以下两款插件：

- WCCrashBlockMonitorPlugin：基于 [KSCrash](https://github.com/kstenerud/KSCrash) 框架开发，具有业界领先的卡顿堆栈捕获能力，同时兼备崩溃捕获能力。
- WCMemoryStatPlugin：一款性能优化到极致的爆内存监控工具，能够全面捕获应用爆内存时的内存分配以及调用堆栈情况。

### 3.5 耗时堆栈提取

子线程检测到主线程 Runloop 时，会获得当前的线程快照当做卡顿文件。但是这个当前的主线程堆栈不一定是最耗时的堆栈，不一定是导致主线程超时的主要原因。

例如，主线程在绘制一个微信logo，过程如下：

<img src="/images/PerfOpt/draw-wechat-logo.png" style="zoom:80%">

子线程在检测到超出阈值时获得的线程快照，主线程的当前任务是“画小气泡”。但其实“画大气泡”才是耗时操作，导致主线程超时的主要原因。**Matrix 卡顿监控通过主线程耗时堆栈提取来解决这个问题。**

卡顿监控定时获取主线程堆栈，并将堆栈保存到内存的一个循环队列中。如下图，每间隔时间 t 获得一个堆栈，然后将堆栈保存到一个最大个数为 3 的循环队列中。有一个游标不断的指向最近的堆栈。

微信的策略是每隔 50 毫秒获取一次主线程堆栈，保存最近 20 个主线程堆栈。这个会增加 3% 的 CPU 占用，内存占用可以忽略不计。

<img src="/images/PerfOpt/time-cost-stack-fetch.png" style="zoom:100%">

当主线程检测到卡顿时，通过对保存到循坏队列中的堆栈进行回溯，获取最近最耗时堆栈。

如下图，检测到卡顿时，内存的循环队列中记录了最近的20个主线程堆栈，需要从中找出最近最耗时的堆栈。Matrix 卡顿监控用如下特征找出最近最耗时堆栈：

- 以栈顶函数为特征，认为栈顶函数相同的即整个堆栈是相同的；
- 取堆栈的间隔是相同的，堆栈的重复次数近似作为堆栈的调用耗时，重复越多，耗时越多；
- 重复次数相同的堆栈可能很有多个，取最近的一个最耗时堆栈。

获得的最近最耗时堆栈会附带到卡顿文件中。

<img src="/images/PerfOpt/stack-back.png" style="zoom:80%">

### 3.6 卡死卡顿

Matrix 中内置了应用被杀原因的检测机制。这个机制从 [Facebook 的博文](https://code.fb.com/ios/reducing-fooms-in-the-facebook-ios-app) 中获得灵感，在其基础上增加了系统强杀的判定。Matrix 检测应用被杀原因的具体机制如下图所示：

<img src="/images/PerfOpt/why-app-killed.png" style="zoom:85%">

**Matrix 检测到应用卡死被强杀，会把应用上次存活时的最后一份卡顿日志标记为卡死卡顿。**

### 3.7 性能损耗

Matrix 卡顿监控不打开耗时堆栈提取，性能损耗可以忽略不计。

打开耗时堆栈提取后，性能损耗和定时获取主线程堆栈的间隔有关。实测，每隔 50 毫秒不断获取主线程堆栈，会增加 3% 的 CPU 占用。

### 3.8 分类方法

直接用 crash report 的分类方法是不行的，这个很好理解：最终卡在 lock 函数的卡顿，外面可能是很多不同的业务，例如可能是读取消息，可能是读取联系人，等等。卡顿监控需要仔细定义自己的分类规则。可以是从调用堆栈的最外层开始归类，或者是取中间一部分归类，或者是取最里面一部分归类。各有优缺点：

- 最外层归类：能够将同一入口的卡顿归类起来。缺点是层数不好定，可能外面十来层都是系统调用，也有可能第一层就是微信的函数了。
- 中间层归类：能够根据事先划分好的“特征值”来归类。缺点是“特征值”不好定，如果要做到自动学习生成的话，对后台分析系统要求太高了。
- 最内层归类：能够将同一原因的卡顿归类起来。缺点是同一分类可能包含不同的业务。

综合考虑并一一尝试之后，我们采用了最内层归类的优化版，亦即进行二级归类。

- 第一级：按照 **最内倒数2层** 归类，这样能够将 **同一原因** 的卡顿集中起来；
  - 第二级分类是从第一级点击进来，然后按照 **最内层倒数4层** 进行归类，这样能够将同一原因，根据 **不同业务(不同入口)** 分散归类起来。

### 3.9 可运营

在正式发布之前，我们进行了灰度，以评估卡顿对用户的影响。收集到的结果是用户平均每天会产生30个 dump 文件，压缩上传大约要 300k 流量。预计正式发布的话会对后台有比较大的压力，对用户也有一定流量损耗。所以必须进行抽样上报。

- 抽样上报：每天抽取不同的用户进行上报，抽样概率是5%。
- 文件上传：被抽中的用户1天仅上传前20个堆栈文件，并且每次上报会进行多文件压缩上传。
- 白名单：对于需要跟进问题的用户，可以在后台配置白名单，强制上报。

另外，为了减少对用户存储空间的影响，卡顿文件仅保存最近7天的记录，过期删除。

## 四、参考链接

- [微信 iOS 卡顿监控系统](https://mp.weixin.qq.com/s?__biz=MzAwNDY1ODY2OQ==&mid=207890859&idx=1&sn=e98dd604cdb854e7a5808d2072c29162&scene=4)
- [微信 Matrix 卡顿监控工具](https://github.com/Tencent/matrix/wiki/Matrix-for-iOS-macOS-%E5%8D%A1%E9%A1%BF%E7%9B%91%E6%8E%A7%E5%8E%9F%E7%90%86) 
- [13 | 如何利用 RunLoop 原理去监控卡顿？](https://time.geekbang.org/column/article/89494)

